{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IwhlCjVtpYj"
      },
      "source": [
        "# MLC-LLM Raw Text Generation in Python\n",
        "\n",
        "Here's a quick overview of how to perform raw text generation in Python. In this tutorial, we will be chatting with the Llama2 model. For the easiest setup, we recommend trying this out in a Google Colab notebook. Click the button below to get started!\n",
        "\n",
        "Raw text generation allows the user to have more flexibility over the prompts, without being forced to create a new conversational template, making prompt customization easier. This serves other demands for APIs to handle LLM generation without the usual system prompts and other items.\n",
        "\n",
        "Learn more about MLC LLM here: https://mlc.ai/mlc-llm/docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsvAL7SSt9Lo"
      },
      "source": [
        "Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_raw_text_generation.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kkADAMCCLi-"
      },
      "source": [
        "## Install MLC LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2EwuS6TCO61"
      },
      "source": [
        "We will start from setting up the environment. First, let us create a new Conda environment, in which we will run the rest of the notebook.\n",
        "\n",
        "```\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojEeEmsqCTPG"
      },
      "source": [
        "**Google Colab**\n",
        "\n",
        "- If you are running this in a Google Colab notebook, you would not need to create a conda environment.\n",
        "- However, be sure to change your runtime to GPU by going to `Runtime` > `Change runtime type` and setting the Hardware accelerator to be \"GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_rX53bGChPn"
      },
      "source": [
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the driver version number as well as what GPUs are currently available for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRPeCflbCij6"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQfVfTAYC1M-"
      },
      "source": [
        "Next, let's download the MLC-AI and mlc-llm nightly build packages. If you are running in a Colab environment, then you can just run the following command. Otherwise, go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi-udt4tC5c9"
      },
      "source": [
        "**Google Colab**: If you are using Colab, you may see the red warnings such as \"You must restart the runtime in order to use newly installed versions.\" For our purpose, we can disregard them, the notebook will still run correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah9tYaCRCkKS"
      },
      "outputs": [],
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-llm-nightly-cu118 -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZGVNJE-DJ9E"
      },
      "source": [
        "Let's confirm we have installed the packages successfully!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6LszJgC7SQ"
      },
      "outputs": [],
      "source": [
        "!python -c \"import tvm; print('tvm installed properly!')\"\n",
        "!python -c \"import mlc_llm; print('mlc_llm installed properly!')\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGfnrRa9DMw1"
      },
      "source": [
        "## Download Prebuilt Models and Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVYkLb0eDjMi"
      },
      "source": [
        "The following commands will download all the available prebuilt libraries (e.g., `.so` files). This may take a while. If in **Google Colab**, you can verify that the files are being downloaded by clicking on the folder icon on the left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg7daEvlD5UB"
      },
      "source": [
        "Note: If you are NOT running in **Google Colab** you may need to run this line `!conda install git git-lfs` to install `git` and `git-lfs` before running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDFbw1KPDLu1"
      },
      "outputs": [],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYqaVjmND7Px"
      },
      "outputs": [],
      "source": [
        "!mkdir -p dist\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt_libs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMEavWCJEC_d"
      },
      "source": [
        "#### Llama-2-7b-chat q4f16_1 prebuilt weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etHEUrfMD8bX"
      },
      "outputs": [],
      "source": [
        "!cd dist && git clone https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q4f16_1-MLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dbHdyfIXHNpo"
      },
      "outputs": [],
      "source": [
        "# Restart colab\n",
        "exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpxrrqyE0S6"
      },
      "source": [
        "## Let's try raw text generation with Llama-2-7b-chat!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VYZtJS_OoCW6"
      },
      "outputs": [],
      "source": [
        "from mlc_llm import ChatModule, ChatConfig, ConvConfig\n",
        "from mlc_llm.callback import StreamToStdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCnYzG1dombI"
      },
      "source": [
        "Use a `ConvConfig` to define the generation settings. Since we will be using the `LM` template, which supports raw text generation, system prompts will not be executed if provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vUn1QHlaoiY8"
      },
      "outputs": [],
      "source": [
        "conv_config = ConvConfig(stop_tokens=[2,], add_bos=True, stop_str=\"[INST]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5460Ca7phM0"
      },
      "source": [
        "Note that `conv_config` is an optional subfield of `chat_config`. The `LM` template serves the basic purposes of raw text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yw0vlNEvpclP"
      },
      "outputs": [],
      "source": [
        "chat_config = ChatConfig(conv_config=conv_config, conv_template=\"LM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UshFruMXpu31"
      },
      "source": [
        "Using the `chat_config` we created, instantiate a `ChatModule`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6AeKjYybpvMH"
      },
      "outputs": [],
      "source": [
        "cm = ChatModule(\n",
        "   model=\"dist/Llama-2-7b-chat-hf-q4f16_1-MLC\",\n",
        "   model_lib_path=\"dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so\",\n",
        "   chat_config=chat_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAbeEqDjqB0T"
      },
      "source": [
        "Let's depict our first prompt. Essentially the LLM will be fed with this exact piece of text, unlike other conversational templates that structure the conversation beforehand to abstract specific settings. However, to make the model follow conversations a chat structure should be provided. Specific tags should be placed, because the model was finetuned with those tags to accurately follow conversations. This allows users to build their own prompts without necessarily building a new template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7_Z_w5VUp7HZ"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"<<SYS>>\\nYou are a helpful, respectful and honest assistant.\\n<</SYS>>\\n\\n\"\n",
        "inst_prompt = \"What is mother nature?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuylQHLQ6ugR"
      },
      "source": [
        "Concatenate system and instruction prompts, and add instruction tags before generation. As you can see, the model will correctly follow the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaVcdEXup8NH",
        "outputId": "631c2f60-68cc-4a90-ecb2-6fc06eb1b642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I'm so glad you asked! Mother Nature is a term used to describe the natural world around us, including all living things and the environment that supports them. It encompasses everything from the tiniest microorganisms to the largest landscapes, and includes all the elements and processes that shape our planet.\n",
            "Mother Nature is the source of all life, providing us with the air we breathe, the water we drink, the food we eat, and the beauty we behold. She is the foundation of our very existence, and yet, she is often taken for granted.\n",
            "It's important to remember that Mother Nature is not just something we rely on for our survival, but she also provides us with endless opportunities for inspiration, creativity, and joy. From the majestic mountains to the rolling hills, from the sparkling oceans to the babbling brooks, Mother Nature offers us a never-ending array of wonders and marvels.\n",
            "So, the next time you take a moment to appreciate the beauty of Mother Nature, remember that you are not just appreciating something beautiful, you are appreciating the very source of your own existence. Take care of her, and she will take care of you.\n"
          ]
        }
      ],
      "source": [
        "output = cm.generate(\n",
        "   prompt=f\"[INST] {system_prompt+inst_prompt} [/INST]\",\n",
        "   progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CijSHO6K9QqG"
      },
      "source": [
        "Structuring the conversation in this way is equivelent to using the following conversational template in MLC-LLM:\n",
        "\n",
        "```cpp\n",
        "Conversation Llama2() {\n",
        "  Conversation conv;\n",
        "  conv.name = \"llama-2\";\n",
        "  conv.system =\n",
        "      (\"[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant.\\n<</SYS>>\\n\\n \");\n",
        "  conv.roles = {\"[INST]\", \"[/INST]\"};\n",
        "  conv.messages = {};\n",
        "  conv.offset = 0;\n",
        "  conv.separator_style = SeparatorStyle::kSepRoleMsg;\n",
        "  conv.seps = {\" \"};\n",
        "  conv.role_msg_sep = \" \";\n",
        "  conv.role_empty_sep = \" \";\n",
        "  conv.stop_tokens = {2};\n",
        "  conv.stop_str = \"[INST]\";\n",
        "  conv.add_bos = true;\n",
        "  return conv;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "008dtOGy7ZMQ"
      },
      "source": [
        "In following case, since we do not add any tags, the model will just follow normal text completion because there isn't a chat structure.\n",
        "\n",
        "**Note:** The `LM` template has no memory, so it will be reset every single generation (as if we would run `cm.reset_chat()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K8X2p7Y61nl",
        "outputId": "7dcc570e-9f62-4744-d4b5-23fa003a4307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "living beings from non-living matter. literally, it is characterized by growth, reproduction, metabolism, response to stimuli, and adaptation to their environment. The concept of life has puzzled scientists and philosophers for centuries, and there is no consensus on a definition that encompasses all aspects of life.\n",
            "The most commonly used definition of life is the \"chemical definition,\" which states that living things are composed of cells, which are the basic structural and functional units of life. Cells are made up of biomolecules such as DNA, RNA, and proteins, which perform a variety of functions necessary for life, such as metabolism, growth, and reproduction.\n",
            "Another definition of life is the \"functional definition,\" which states that living things have the ability to maintain homeostasis, or a stable internal environment, despite changes in the external environment. This means that living things are able to regulate their internal processes and maintain a stable balance of chemical and physical parameters, such as temperature, pH, and concentration of nutrients and waste products.\n",
            "A third definition of life is the \"process definition,\" which states that living things are characterized by a set of processes that are unique to living things and cannot be replicated by non-living matter. These processes include metabolism, growth, reproduction, response to stimuli, and adaptation to their environment.\n",
            "There are also other definitions of life, such as the \"energy definition,\" which states that living things are characterized by their ability to capture and convert energy from their environment, and the \"information definition,\" which states that living things are characterized by their ability to store, process, and transmit information.\n",
            "Despite these various definitions, there is still much debate among scientists and philosophers about what exactly constitutes life. Some argue that life is a fundamental property of the universe, while others believe that it is a product of historical and cultural factors. Ultimately, the definition of life is likely to be complex and multifaceted, encompassing a variety of biological, chemical, and physical processes that are unique to living things.\n"
          ]
        }
      ],
      "source": [
        "output = cm.generate(\n",
        "   prompt=\"Life is a quality that distinguishes\",\n",
        "   progress_callback=StreamToStdout(callback_interval=2),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
