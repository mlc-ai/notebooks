{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with MLC-LLM using the Llama 2 Model\n",
    "\n",
    "Here's a quick overview of how to get started with the MLC-LLM `ChatModule` in Python. In this tutorial, we will chat with the [Llama 2](https://ai.meta.com/llama/) model. For the easiest setup, we recommend trying this out in a Google Colab notebook. Click the button below to get started!\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Let's set up your environment, so you can successfully run the `ChatModule`. First, lets set up the Conda environment which we'll be running this notebook in.\n",
    "\n",
    "```bash\n",
    "conda create --name mlc-llm python=3.10\n",
    "conda activate mlc-llm\n",
    "```\n",
    "\n",
    "**Google Colab:** If you are running this in a Google Colab notebook, be sure to change your runtime to GPU by going to Runtime > Change runtime type and setting the Hardware accelerator to be \"GPU\". Select \"Connect\" on the top right to instantiate your GPU session.\n",
    "\n",
    "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Thu_Nov_18_09:45:30_PST_2021\n",
      "Cuda compilation tools, release 11.5, V11.5.119\n",
      "Build cuda_11.5.r11.5/compiler.30672275_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab:** If you are running this in a Google Colab notebook, you will also need to download some Vulkan drivers. You may not need to download the drivers if you are running this locally and already have Vulkan support (or are not using Vulkan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install -y vulkan-tools libnvidia-gl-525"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Colab:** You can run the following command to confirm that the Vulkan drivers have installed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vulkaninfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's download the MLC-AI and MLC-Chat nightly build packages. Go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://mlc.ai/wheels\n",
      "Collecting mlc-ai-nightly-cu116\n",
      "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu116-0.12.dev1297-cp310-cp310-manylinux_2_28_x86_64.whl (81.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mlc-chat-nightly-cu116\n",
      "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu116-0.1.dev278-cp310-cp310-manylinux_2_28_x86_64.whl (20.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.4/20.4 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu116)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting cloudpickle (from mlc-ai-nightly-cu116)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting decorator (from mlc-ai-nightly-cu116)\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting ml-dtypes (from mlc-ai-nightly-cu116)\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting numpy (from mlc-ai-nightly-cu116)\n",
      "  Using cached numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "Collecting psutil (from mlc-ai-nightly-cu116)\n",
      "  Using cached psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "Collecting scipy (from mlc-ai-nightly-cu116)\n",
      "  Using cached scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "Collecting tornado (from mlc-ai-nightly-cu116)\n",
      "  Using cached tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "Collecting typing-extensions (from mlc-ai-nightly-cu116)\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting fastapi (from mlc-chat-nightly-cu116)\n",
      "  Using cached fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
      "Collecting uvicorn (from mlc-chat-nightly-cu116)\n",
      "  Using cached uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
      "Collecting shortuuid (from mlc-chat-nightly-cu116)\n",
      "  Using cached shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached pydantic-2.0.3-py3-none-any.whl (364 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Collecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu116)\n",
      "  Using cached click-8.1.6-py3-none-any.whl (97 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu116)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Collecting pydantic-core==2.3.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached pydantic_core-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "Collecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu116)\n",
      "  Using cached exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu116, fastapi, mlc-chat-nightly-cu116\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.3.2\n",
      "    Uninstalling tornado-6.3.2:\n",
      "      Successfully uninstalled tornado-6.3.2\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.0\n",
      "    Uninstalling sniffio-1.3.0:\n",
      "      Successfully uninstalled sniffio-1.3.0\n",
      "  Attempting uninstall: shortuuid\n",
      "    Found existing installation: shortuuid 1.0.11\n",
      "    Uninstalling shortuuid-1.0.11:\n",
      "      Successfully uninstalled shortuuid-1.0.11\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.5\n",
      "    Uninstalling psutil-5.9.5:\n",
      "      Successfully uninstalled psutil-5.9.5\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.1\n",
      "    Uninstalling numpy-1.25.1:\n",
      "      Successfully uninstalled numpy-1.25.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.1.2\n",
      "    Uninstalling exceptiongroup-1.1.2:\n",
      "      Successfully uninstalled exceptiongroup-1.1.2\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.1\n",
      "    Uninstalling cloudpickle-2.2.1:\n",
      "      Successfully uninstalled cloudpickle-2.2.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.6\n",
      "    Uninstalling click-8.1.6:\n",
      "      Successfully uninstalled click-8.1.6\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.5.0\n",
      "    Uninstalling annotated-types-0.5.0:\n",
      "      Successfully uninstalled annotated-types-0.5.0\n",
      "  Attempting uninstall: uvicorn\n",
      "    Found existing installation: uvicorn 0.23.1\n",
      "    Uninstalling uvicorn-0.23.1:\n",
      "      Successfully uninstalled uvicorn-0.23.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.1\n",
      "    Uninstalling scipy-1.11.1:\n",
      "      Successfully uninstalled scipy-1.11.1\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.3.0\n",
      "    Uninstalling pydantic_core-2.3.0:\n",
      "      Successfully uninstalled pydantic_core-2.3.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.7.1\n",
      "    Uninstalling anyio-3.7.1:\n",
      "      Successfully uninstalled anyio-3.7.1\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.27.0\n",
      "    Uninstalling starlette-0.27.0:\n",
      "      Successfully uninstalled starlette-0.27.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.0.3\n",
      "    Uninstalling pydantic-2.0.3:\n",
      "      Successfully uninstalled pydantic-2.0.3\n",
      "  Attempting uninstall: mlc-ai-nightly-cu116\n",
      "    Found existing installation: mlc-ai-nightly-cu116 0.12.dev1297\n",
      "    Uninstalling mlc-ai-nightly-cu116-0.12.dev1297:\n",
      "      Successfully uninstalled mlc-ai-nightly-cu116-0.12.dev1297\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.100.0\n",
      "    Uninstalling fastapi-0.100.0:\n",
      "      Successfully uninstalled fastapi-0.100.0\n",
      "  Attempting uninstall: mlc-chat-nightly-cu116\n",
      "    Found existing installation: mlc-chat-nightly-cu116 0.1.dev278\n",
      "    Uninstalling mlc-chat-nightly-cu116-0.1.dev278:\n",
      "      Successfully uninstalled mlc-chat-nightly-cu116-0.1.dev278\n",
      "Successfully installed annotated-types-0.5.0 anyio-3.7.1 attrs-23.1.0 click-8.1.6 cloudpickle-2.2.1 decorator-5.1.1 exceptiongroup-1.1.2 fastapi-0.100.0 h11-0.14.0 idna-3.4 ml-dtypes-0.2.0 mlc-ai-nightly-cu116-0.12.dev1297 mlc-chat-nightly-cu116-0.1.dev278 numpy-1.25.1 psutil-5.9.5 pydantic-2.0.3 pydantic-core-2.3.0 scipy-1.11.1 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.2 typing-extensions-4.7.1 uvicorn-0.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre --force-reinstall mlc-ai-nightly-cu116 mlc-chat-nightly-cu116 -f https://mlc.ai/wheels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can clone the [MLC-LLM project](https://github.com/mlc-ai/mlc-llm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlc-llm'...\n",
      "remote: Enumerating objects: 5287, done.\u001b[K\n",
      "remote: Counting objects: 100% (1289/1289), done.\u001b[K\n",
      "remote: Compressing objects: 100% (389/389), done.\u001b[K\n",
      "remote: Total 5287 (delta 1013), reused 1031 (delta 897), pack-reused 3998\u001b[K\n",
      "Receiving objects: 100% (5287/5287), 19.94 MiB | 16.42 MiB/s, done.\n",
      "Resolving deltas: 100% (3323/3323), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mlc-ai/mlc-llm.git\n",
    "!cd mlc-llm && git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's download the model weights for the Llama 2 model and the prebuilt model libraries from Github. In order to download the large weights, we'll have to use `git lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 23.5.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!conda install git git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlc-llm/dist/prebuilt/lib'...\n",
      "remote: Enumerating objects: 154, done.\u001b[K\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 154 (delta 7), reused 14 (delta 4), pack-reused 135\u001b[K\n",
      "Receiving objects: 100% (154/154), 41.48 MiB | 18.77 MiB/s, done.\n",
      "Resolving deltas: 100% (100/100), done.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p mlc-llm/dist/prebuilt\n",
    "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git mlc-llm/dist/prebuilt/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlc-chat-Llama-2-7b-chat-hf-q4f16_1'...\n",
      "remote: Enumerating objects: 126, done.\u001b[K\n",
      "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
      "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
      "remote: Total 126 (delta 0), reused 123 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (126/126), 497.08 KiB | 9.04 MiB/s, done.\n",
      "Filtering content: 100% (116/116), 3.53 GiB | 33.14 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!cd mlc-llm/dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Chat\n",
    "\n",
    "Before we can chat with the model, we must first import a few libraries and instantiate a `ChatModule` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlc_chat import ChatModule\n",
    "import tvm\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must invoke the `ChatModule` with the appropriate device type, such as `vulkan`, `cuda`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ChatModule(target=\"vulkan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load the model weights and prebuilt model library into the `ChatModule`, we have to first call the `reload` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = tvm.runtime.load_module(\"mlc-llm/dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-vulkan.so\")\n",
    "cm.reload(lib=lib, model_path=\"mlc-llm/dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all that's needed to set up the `ChatModule`. You can now chat with the model by inputting any prompt you'd like. Try it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here is a poem I came up with based on the theme of \"nature\":\n",
      "\n",
      "In the grand expanse of nature's embrace,\n",
      "Where trees stretch high and skies are wide and vast,\n",
      "The world is full of beauty, full of grace.\n",
      "A place where creatures roam and plants bloom with haste.\n",
      "\n",
      "From the tiniest flower to the loftiest tree,\n",
      "Each one unique, a work of art divine,\n",
      "Nature's handiwork, a masterpiece to see.\n",
      "A world where life thrives, where love and light shine.\n",
      "\n",
      "In nature's arms, we find our peaceful place,\n",
      "Where worries fade and joy takes their space,\n",
      "A sanctuary where we can find our grace.\n",
      "A place where love and wonder fill the space.\n",
      "\n",
      "Please let me know if you would like me to modify the poem in any way!\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Prompt: \")\n",
    "cm.prefill(input=prompt)\n",
    "\n",
    "msg = None\n",
    "while not cm.stopped():\n",
    "    cm.decode()\n",
    "    msg = cm.get_message()\n",
    "    clear_output()\n",
    "    print(msg, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the speed of the chat bot, you can print some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prefill: 42.5 tok/s, decode: 81.3 tok/s'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.runtime_stats_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `ChatModule` will keep a history of your chat. You can reset the chat history by running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.reset_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
