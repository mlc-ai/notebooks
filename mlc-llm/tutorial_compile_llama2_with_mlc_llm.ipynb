{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Compiling Llama-2 with MLC-LLM in Python\n",
        "\n",
        "This notebook demonstrates how to compile a model via [MLC-LLM](https://github.com/mlc-ai/mlc-llm) with a Python API. The `mlc-llm` package allows you to compile model at any directory. In this tutorial, we will compile the newly released Llama-2, then chat with the model we build. You could also chat with [many other models](https://mlc.ai/mlc-llm/docs/compilation/compile_models.html#more-model-compile-commands) with the same method.\n",
        "\n",
        "Note that you could also compile models in command line (as opposed to Python), as shown in [the docs](https://mlc.ai/mlc-llm/docs/compilation/compile_models.html).\n",
        "\n",
        "If you are interested in learning about how the compilation works behind the scene, you may find [this course on machine learning compilation](https://mlc.ai/) helpful."
      ],
      "metadata": {
        "id": "ABn8o4hYujp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_compile_llama2_with_mlc_llm.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "CYttF0PzskcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "\n",
        "We will start from setting up the environment. First, let us create a new Conda environment, in which we will run the rest of the notebook.\n",
        "\n",
        "```bash\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```"
      ],
      "metadata": {
        "id": "OOsonPojw5r8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**\n",
        "- If you are running this in a Google Colab notebook, you would not need to create a conda environment.\n",
        "- However, be sure to change your runtime to GPU by going to `Runtime` > `Change runtime type` and setting the Hardware accelerator to be \"GPU\".\n",
        "- Besides, compiling Llama-2 **may** require more RAM than the default Colab allocates. You may need to either upgrade Colab to a paid plan (so that `runtime shape` can be set to `High RAM`), or use other environments.\n",
        "  - But we also notice that, sometimes rerunning it several times (just the build portion) without exceeding the default RAM amount."
      ],
      "metadata": {
        "id": "NQChv9H2jAmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the driver version number as well as what GPUs are currently available for use."
      ],
      "metadata": {
        "id": "RR6IKtENjSe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMA8yJTXjVK4",
        "outputId": "e7d17fcb-727a-4e3d-d4fd-234467d94804"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 24 04:57:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. If you are running in a Colab environment, then you can just run the following command. Otherwise, go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ],
      "metadata": {
        "id": "S8fC3U6kxr-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: If you are using Colab, you may see the red warnings such as **\"You must restart the runtime in order to use newly installed versions.\"** For our purpose, we can disregard them, the notebook will still run correctly."
      ],
      "metadata": {
        "id": "aJvP-f65oF4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"
      ],
      "metadata": {
        "id": "buQYWuX4Dvqd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6de2a6b4-1d6b-40d4-9be9-e556a86069f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu118-0.12.dev1300-cp310-cp310-manylinux_2_28_x86_64.whl (81.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mlc-chat-nightly-cu118\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_chat_nightly_cu118-0.1.dev288-cp310-cp310-manylinux_2_28_x86_64.whl (20.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.4/20.4 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs (from mlc-ai-nightly-cu118)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle (from mlc-ai-nightly-cu118)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting decorator (from mlc-ai-nightly-cu118)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting ml-dtypes (from mlc-ai-nightly-cu118)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from mlc-ai-nightly-cu118)\n",
            "  Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil (from mlc-ai-nightly-cu118)\n",
            "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from mlc-ai-nightly-cu118)\n",
            "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tornado (from mlc-ai-nightly-cu118)\n",
            "  Downloading tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from mlc-ai-nightly-cu118)\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting fastapi (from mlc-chat-nightly-cu118)\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from mlc-chat-nightly-cu118)\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid (from mlc-chat-nightly-cu118)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading pydantic-2.0.3-py3-none-any.whl (364 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.0/364.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click>=7.0 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn->mlc-chat-nightly-cu118)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
            "Collecting pydantic-core==2.3.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading pydantic_core-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0 (from starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna>=2.8 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio>=1.1 (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->mlc-chat-nightly-cu118)\n",
            "  Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: typing-extensions, tornado, sniffio, shortuuid, psutil, numpy, idna, h11, exceptiongroup, decorator, cloudpickle, click, attrs, annotated-types, uvicorn, scipy, pydantic-core, ml-dtypes, anyio, starlette, pydantic, mlc-ai-nightly-cu118, fastapi, mlc-chat-nightly-cu118\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.3.1\n",
            "    Uninstalling tornado-6.3.1:\n",
            "      Successfully uninstalled tornado-6.3.1\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: exceptiongroup\n",
            "    Found existing installation: exceptiongroup 1.1.2\n",
            "    Uninstalling exceptiongroup-1.1.2:\n",
            "      Successfully uninstalled exceptiongroup-1.1.2\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.6\n",
            "    Uninstalling click-8.1.6:\n",
            "      Successfully uninstalled click-8.1.6\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.11\n",
            "    Uninstalling pydantic-1.10.11:\n",
            "      Successfully uninstalled pydantic-1.10.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "confection 0.1.0 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.3.1, but you have tornado 6.3.2 which is incompatible.\n",
            "inflect 6.0.5 requires pydantic<2,>=1.9.1, but you have pydantic 2.0.3 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\n",
            "spacy 3.5.4 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.0.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.1 which is incompatible.\n",
            "thinc 8.1.10 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.5.0 anyio-3.7.1 attrs-23.1.0 click-8.1.6 cloudpickle-2.2.1 decorator-5.1.1 exceptiongroup-1.1.2 fastapi-0.100.0 h11-0.14.0 idna-3.4 ml-dtypes-0.2.0 mlc-ai-nightly-cu118-0.12.dev1300 mlc-chat-nightly-cu118-0.1.dev288 numpy-1.25.1 psutil-5.9.5 pydantic-2.0.3 pydantic-core-2.3.0 scipy-1.11.1 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 tornado-6.3.2 typing-extensions-4.7.1 uvicorn-0.23.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator",
                  "numpy",
                  "psutil",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab**: Since we ignored the warnings/errors in the previous cell, run the following cell to verify the installation did in fact occur properly."
      ],
      "metadata": {
        "id": "jO3bmbFvntZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import tvm; print('tvm installed properly!')\"\n",
        "!python -c \"import mlc_chat; print('mlc_chat installed properly!')\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wEUknFrns8Q",
        "outputId": "6846e76a-56b3-46bf-fd8a-507851ae7776"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tvm installed properly!\n",
            "mlc_chat installed properly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we clone the [mlc-llm repository](https://github.com/mlc-ai/mlc-llm).\n",
        "\n",
        "**Google Colab**: Note, this will isntall into the mlc-llm folder. You can click the folder icon on the left menu bar to see the local file system and verify that the repository was cloned successfully."
      ],
      "metadata": {
        "id": "OZpxlmX2d7g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/mlc-ai/mlc-llm.git"
      ],
      "metadata": {
        "id": "O5trBYo7xlJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c583a47a-4104-4ba2-fecd-d2122148be58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlc-llm'...\n",
            "remote: Enumerating objects: 5432, done.\u001b[K\n",
            "remote: Counting objects: 100% (1435/1435), done.\u001b[K\n",
            "remote: Compressing objects: 100% (441/441), done.\u001b[K\n",
            "remote: Total 5432 (delta 1106), reused 1161 (delta 989), pack-reused 3997\u001b[K\n",
            "Receiving objects: 100% (5432/5432), 20.04 MiB | 17.49 MiB/s, done.\n",
            "Resolving deltas: 100% (3413/3413), done.\n",
            "Submodule '3rdparty/argparse' (https://github.com/p-ranav/argparse) registered for path '3rdparty/argparse'\n",
            "Submodule '3rdparty/googletest' (https://github.com/google/googletest.git) registered for path '3rdparty/googletest'\n",
            "Submodule '3rdparty/tokenizers-cpp' (https://github.com/mlc-ai/tokenizers-cpp) registered for path '3rdparty/tokenizers-cpp'\n",
            "Submodule '3rdparty/tvm' (https://github.com/mlc-ai/relax.git) registered for path '3rdparty/tvm'\n",
            "Cloning into '/content/mlc-llm/3rdparty/argparse'...\n",
            "remote: Enumerating objects: 2421, done.        \n",
            "remote: Counting objects: 100% (27/27), done.        \n",
            "remote: Compressing objects: 100% (10/10), done.        \n",
            "remote: Total 2421 (delta 10), reused 23 (delta 9), pack-reused 2394        \n",
            "Receiving objects: 100% (2421/2421), 740.39 KiB | 18.51 MiB/s, done.\n",
            "Resolving deltas: 100% (1302/1302), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/googletest'...\n",
            "remote: Enumerating objects: 26653, done.        \n",
            "remote: Counting objects: 100% (227/227), done.        \n",
            "remote: Compressing objects: 100% (119/119), done.        \n",
            "remote: Total 26653 (delta 136), reused 152 (delta 94), pack-reused 26426        \n",
            "Receiving objects: 100% (26653/26653), 12.44 MiB | 18.02 MiB/s, done.\n",
            "Resolving deltas: 100% (19777/19777), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tokenizers-cpp'...\n",
            "remote: Enumerating objects: 93, done.        \n",
            "remote: Counting objects: 100% (93/93), done.        \n",
            "remote: Compressing objects: 100% (78/78), done.        \n",
            "remote: Total 93 (delta 28), reused 66 (delta 12), pack-reused 0        \n",
            "Receiving objects: 100% (93/93), 28.79 KiB | 7.20 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm'...\n",
            "remote: Enumerating objects: 186285, done.        \n",
            "remote: Counting objects: 100% (10986/10986), done.        \n",
            "remote: Compressing objects: 100% (1432/1432), done.        \n",
            "remote: Total 186285 (delta 9684), reused 10493 (delta 9477), pack-reused 175299        \n",
            "Receiving objects: 100% (186285/186285), 69.02 MiB | 12.67 MiB/s, done.\n",
            "Resolving deltas: 100% (145157/145157), done.\n",
            "Submodule path '3rdparty/argparse': checked out '557948f1236db9e27089959de837cc23de6c6bbd'\n",
            "Submodule path '3rdparty/googletest': checked out '45804691223635953f311cf31a10c632553bbfc3'\n",
            "Submodule path '3rdparty/tokenizers-cpp': checked out '6e3a37e2ce4165fb70635a684ad300034fcb63dc'\n",
            "Submodule 'sentencepiece' (https://github.com/google/sentencepiece) registered for path '3rdparty/tokenizers-cpp/sentencepiece'\n",
            "Cloning into '/content/mlc-llm/3rdparty/tokenizers-cpp/sentencepiece'...\n",
            "remote: Enumerating objects: 4815, done.        \n",
            "remote: Counting objects: 100% (1478/1478), done.        \n",
            "remote: Compressing objects: 100% (322/322), done.        \n",
            "remote: Total 4815 (delta 1202), reused 1221 (delta 1116), pack-reused 3337        \n",
            "Receiving objects: 100% (4815/4815), 26.73 MiB | 16.21 MiB/s, done.\n",
            "Resolving deltas: 100% (3311/3311), done.\n",
            "Submodule path '3rdparty/tokenizers-cpp/sentencepiece': checked out 'f2219b53e24ff5deee4cacdc2d0ca3074e529a07'\n",
            "Submodule path '3rdparty/tvm': checked out '481ff8696fc97de0119e8068ce99da3870d92db9'\n",
            "Submodule '3rdparty/OpenCL-Headers' (https://github.com/KhronosGroup/OpenCL-Headers.git) registered for path '3rdparty/tvm/3rdparty/OpenCL-Headers'\n",
            "Submodule '3rdparty/cnpy' (https://github.com/rogersce/cnpy.git) registered for path '3rdparty/tvm/3rdparty/cnpy'\n",
            "Submodule '3rdparty/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path '3rdparty/tvm/3rdparty/cutlass'\n",
            "Submodule '3rdparty/cutlass_fpA_intB_gemm' (https://github.com/tlc-pack/cutlass_fpA_intB_gemm) registered for path '3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm'\n",
            "Submodule 'dlpack' (https://github.com/dmlc/dlpack.git) registered for path '3rdparty/tvm/3rdparty/dlpack'\n",
            "Submodule 'dmlc-core' (https://github.com/dmlc/dmlc-core.git) registered for path '3rdparty/tvm/3rdparty/dmlc-core'\n",
            "Submodule '3rdparty/libbacktrace' (https://github.com/tlc-pack/libbacktrace.git) registered for path '3rdparty/tvm/3rdparty/libbacktrace'\n",
            "Submodule '3rdparty/rang' (https://github.com/agauniyal/rang.git) registered for path '3rdparty/tvm/3rdparty/rang'\n",
            "Submodule '3rdparty/vta-hw' (https://github.com/apache/tvm-vta.git) registered for path '3rdparty/tvm/3rdparty/vta-hw'\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/OpenCL-Headers'...\n",
            "remote: Enumerating objects: 1276, done.        \n",
            "remote: Counting objects: 100% (299/299), done.        \n",
            "remote: Compressing objects: 100% (119/119), done.        \n",
            "remote: Total 1276 (delta 244), reused 199 (delta 177), pack-reused 977        \n",
            "Receiving objects: 100% (1276/1276), 742.53 KiB | 15.80 MiB/s, done.\n",
            "Resolving deltas: 100% (828/828), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/cnpy'...\n",
            "remote: Enumerating objects: 164, done.        \n",
            "remote: Total 164 (delta 0), reused 0 (delta 0), pack-reused 164        \n",
            "Receiving objects: 100% (164/164), 52.32 KiB | 8.72 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/cutlass'...\n",
            "remote: Enumerating objects: 21027, done.        \n",
            "remote: Counting objects: 100% (5068/5068), done.        \n",
            "remote: Compressing objects: 100% (461/461), done.        \n",
            "remote: Total 21027 (delta 4729), reused 4629 (delta 4604), pack-reused 15959        \n",
            "Receiving objects: 100% (21027/21027), 30.72 MiB | 17.65 MiB/s, done.\n",
            "Resolving deltas: 100% (15652/15652), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm'...\n",
            "remote: Enumerating objects: 292, done.        \n",
            "remote: Counting objects: 100% (292/292), done.        \n",
            "remote: Compressing objects: 100% (122/122), done.        \n",
            "remote: Total 292 (delta 170), reused 270 (delta 151), pack-reused 0        \n",
            "Receiving objects: 100% (292/292), 129.28 KiB | 11.75 MiB/s, done.\n",
            "Resolving deltas: 100% (170/170), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/dlpack'...\n",
            "remote: Enumerating objects: 462, done.        \n",
            "remote: Counting objects: 100% (99/99), done.        \n",
            "remote: Compressing objects: 100% (40/40), done.        \n",
            "remote: Total 462 (delta 74), reused 69 (delta 59), pack-reused 363        \n",
            "Receiving objects: 100% (462/462), 1.70 MiB | 22.36 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/dmlc-core'...\n",
            "remote: Enumerating objects: 6294, done.        \n",
            "remote: Counting objects: 100% (158/158), done.        \n",
            "remote: Compressing objects: 100% (111/111), done.        \n",
            "remote: Total 6294 (delta 67), reused 98 (delta 30), pack-reused 6136        \n",
            "Receiving objects: 100% (6294/6294), 1.68 MiB | 23.26 MiB/s, done.\n",
            "Resolving deltas: 100% (3813/3813), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/libbacktrace'...\n",
            "remote: Enumerating objects: 459, done.        \n",
            "remote: Counting objects: 100% (311/311), done.        \n",
            "remote: Compressing objects: 100% (36/36), done.        \n",
            "remote: Total 459 (delta 285), reused 275 (delta 275), pack-reused 148        \n",
            "Receiving objects: 100% (459/459), 1.03 MiB | 4.23 MiB/s, done.\n",
            "Resolving deltas: 100% (341/341), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/rang'...\n",
            "remote: Enumerating objects: 735, done.        \n",
            "remote: Counting objects: 100% (31/31), done.        \n",
            "remote: Compressing objects: 100% (27/27), done.        \n",
            "remote: Total 735 (delta 9), reused 15 (delta 3), pack-reused 704        \n",
            "Receiving objects: 100% (735/735), 265.43 KiB | 584.00 KiB/s, done.\n",
            "Resolving deltas: 100% (371/371), done.\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/vta-hw'...\n",
            "remote: Enumerating objects: 3312, done.        \n",
            "remote: Counting objects: 100% (328/328), done.        \n",
            "remote: Compressing objects: 100% (137/137), done.        \n",
            "remote: Total 3312 (delta 256), reused 191 (delta 191), pack-reused 2984        \n",
            "Receiving objects: 100% (3312/3312), 1.43 MiB | 15.74 MiB/s, done.\n",
            "Resolving deltas: 100% (1443/1443), done.\n",
            "Submodule path '3rdparty/tvm/3rdparty/OpenCL-Headers': checked out 'b590a6bfe034ea3a418b7b523e3490956bcb367a'\n",
            "Submodule path '3rdparty/tvm/3rdparty/cnpy': checked out '4e8810b1a8637695171ed346ce68f6984e585ef4'\n",
            "Submodule path '3rdparty/tvm/3rdparty/cutlass': checked out '146d314057c5f193a70c2b36896e739c8c60aef4'\n",
            "Submodule path '3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm': checked out '390e821fbad2356089aab603d7116c6c820eae65'\n",
            "Submodule 'cutlass' (https://github.com/NVIDIA/cutlass) registered for path '3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass'\n",
            "Cloning into '/content/mlc-llm/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass'...\n",
            "remote: Enumerating objects: 21027, done.        \n",
            "remote: Counting objects: 100% (5110/5110), done.        \n",
            "remote: Compressing objects: 100% (449/449), done.        \n",
            "remote: Total 21027 (delta 4774), reused 4682 (delta 4658), pack-reused 15917        \n",
            "Receiving objects: 100% (21027/21027), 30.71 MiB | 13.23 MiB/s, done.\n",
            "Resolving deltas: 100% (15652/15652), done.\n",
            "Submodule path '3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass': checked out 'cc85b64cf676c45f98a17e3a47c0aafcf817f088'\n",
            "Submodule path '3rdparty/tvm/3rdparty/dlpack': checked out 'e2bdd3bee8cb6501558042633fa59144cc8b7f5f'\n",
            "Submodule path '3rdparty/tvm/3rdparty/dmlc-core': checked out '09511cf9fe5ff103900a5eafb50870dc84cc17c8'\n",
            "Submodule path '3rdparty/tvm/3rdparty/libbacktrace': checked out '08f7c7e69f8ea61a0c4151359bc8023be8e9217b'\n",
            "Submodule path '3rdparty/tvm/3rdparty/rang': checked out 'cabe04d6d6b05356fa8f9741704924788f0dd762'\n",
            "Submodule path '3rdparty/tvm/3rdparty/vta-hw': checked out '36a91576edf633479c78649e050f18dd2ddc8103'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then install `mlc-llm` as a package, so that we can use its functions outside of this directory."
      ],
      "metadata": {
        "id": "XiVKUPASeQ-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mlc-llm && pip install -e . && cd -"
      ],
      "metadata": {
        "id": "loNmGvcfw40k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ef77da-f77c-4d21-f15e-258f5ec23e60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/mlc-llm\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-llm==0.1.dev290+gd4c3a17) (1.25.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mlc-llm==0.1.dev290+gd4c3a17) (2.0.1+cu118)\n",
            "Collecting transformers (from mlc-llm==0.1.dev290+gd4c3a17)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-llm==0.1.dev290+gd4c3a17) (1.11.1)\n",
            "Collecting timm (from mlc-llm==0.1.dev290+gd4c3a17)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->mlc-llm==0.1.dev290+gd4c3a17) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->mlc-llm==0.1.dev290+gd4c3a17) (6.0.1)\n",
            "Collecting huggingface-hub (from timm->mlc-llm==0.1.dev290+gd4c3a17)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm->mlc-llm==0.1.dev290+gd4c3a17)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev290+gd4c3a17) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev290+gd4c3a17) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev290+gd4c3a17) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev290+gd4c3a17) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev290+gd4c3a17) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->mlc-llm==0.1.dev290+gd4c3a17) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->mlc-llm==0.1.dev290+gd4c3a17) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->mlc-llm==0.1.dev290+gd4c3a17) (16.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc-llm==0.1.dev290+gd4c3a17) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc-llm==0.1.dev290+gd4c3a17) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mlc-llm==0.1.dev290+gd4c3a17) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->mlc-llm==0.1.dev290+gd4c3a17)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mlc-llm==0.1.dev290+gd4c3a17) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->mlc-llm==0.1.dev290+gd4c3a17) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mlc-llm==0.1.dev290+gd4c3a17) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc-llm==0.1.dev290+gd4c3a17) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc-llm==0.1.dev290+gd4c3a17) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc-llm==0.1.dev290+gd4c3a17) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mlc-llm==0.1.dev290+gd4c3a17) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mlc-llm==0.1.dev290+gd4c3a17) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm->mlc-llm==0.1.dev290+gd4c3a17) (8.4.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, timm, mlc-llm\n",
            "  Running setup.py develop for mlc-llm\n",
            "Successfully installed huggingface-hub-0.16.4 mlc-llm-0.1.dev290+gd4c3a17 safetensors-0.3.1 timm-0.9.2 tokenizers-0.13.3 transformers-4.31.0\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Llama-2 model\n",
        "After setting up the environment, we need to download the model we will compile. In this case, it would be [Llama-2-7B-Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). Note: you do not need to download from this link, we will download the model for you in this notebook."
      ],
      "metadata": {
        "id": "uYxdGt28eaCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate that we can compile models using the `mlc-llm` model anywhere, we will create a separate directory to perform our work."
      ],
      "metadata": {
        "id": "HZxG6q2Ae2E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./my_workspace && ls"
      ],
      "metadata": {
        "id": "X3v2WG2B0bPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7b8a70-6edc-4776-b1d3-22b67342fd78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mlc-llm  my_workspace  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd my_workspace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u0XJO1u8WYM",
        "outputId": "d6ed29f1-c92a-4cc9-fa7b-234526394bf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/my_workspace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to download the large weights, we'll have to use `git lfs`."
      ],
      "metadata": {
        "id": "Rax6nMTnj4NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "id": "SFmQ11_Lj6xK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4184d16a-605f-4188-dcd3-93fe625e1a62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will download the Llama-2 7B model from huggingface. Please first [request for access](https://huggingface.co/meta-llama) to Llama-2 weights (i.e. click [Llama-2 7B](https://huggingface.co/meta-llama/Llama-2-7b) and click the button to request access to the repo near the top of the model card information) from Meta using the email of your huggingface account. Then your huggingface account will have access to the model.\n",
        "\n",
        "Since this particular model requires permission, we would need to log in to our huggingface account. In order to \"log in\" to your hugginface account on Colab or notebooks, you would need to create an [Access Token](https://huggingface.co/settings/tokens), and copy the token into when prompted below.\n",
        "\n",
        "(Note: if the command appears to be taking a long time that most likely means the model is being downloaded, please check your filesystem to see if the directory `Llama-2-7b-chat-hf` has been created and is being populated)"
      ],
      "metadata": {
        "id": "tGp3V7EJfDyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass, subprocess\n",
        "command = ['git', 'clone', f'https://{input(\"Enter your huggingface username: \")}:{getpass.getpass(prompt=\"Huggingface CLI Access Token: \")}@huggingface.co/meta-llama/Llama-2-7b-chat-hf']\n",
        "p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "command = []\n",
        "while p.poll() is None:\n",
        "  l = p.stderr.readline()\n",
        "  print(l.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcrGSV7uZS5G",
        "outputId": "b447f436-fc86-4c71-8a37-8cb7ee6a43bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your huggingface username: CharlieRuan0130\n",
            "Huggingface CLI Access Token: ··········\n",
            "Cloning into 'Llama-2-7b-chat-hf'...\n",
            "\n",
            "Filtering content: 100% (5/5), 9.10 GiB | 8.07 MiB/s, done.\n",
            "\n",
            "Encountered 2 file(s) that may not have been copied correctly on Windows:\n",
            "\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "\tmodel-00001-of-00002.safetensors\n",
            "\n",
            "\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab:** If you have the free version of Colab, you will most likely run out of disk space, so please run the following command to free up some disk space."
      ],
      "metadata": {
        "id": "eeUK-5yP3gRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Llama-2-7b-chat-hf/*.safetensors"
      ],
      "metadata": {
        "id": "Eaw4In763fur"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile Llama2 with `mlc_llm`\n",
        "\n",
        "Finally, we can compile the model we just downloaded in Python."
      ],
      "metadata": {
        "id": "cHZJG2e5gGEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to restart runtime since notebooks cannot find the module right after installing\n",
        "# Simply run this cell, then run the next cells after runtime finishes restarting\n",
        "exit()"
      ],
      "metadata": {
        "id": "snyN3kKvFdTh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After restarting the runtime of the notebook, first go into the workspace we created. After this cell, all code below will be in Python!"
      ],
      "metadata": {
        "id": "rXF2fU-7ktsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd my_workspace"
      ],
      "metadata": {
        "id": "HdFkznAkkr5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b75d3c-bbf2-48d8-fde7-96302bd983af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/my_workspace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import `mlc_llm` that we installed using `pip -p`. `mlc_chat` and `tvm` are included in the nightly pacakges we installed earlier."
      ],
      "metadata": {
        "id": "nqrdTQFZkfNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlc_llm, mlc_chat, tvm"
      ],
      "metadata": {
        "id": "8pASvBuVDQms"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a dataclass `BuildArgs` to organize the arguments for building the model. Besides specifying the model with `model` (when the weight is local), you could also use the argument `hf_path`, which will download the model from huggingface directly.\n",
        "\n",
        "For more details on the arguments, please see [the docs for the CLI](https://mlc.ai/mlc-llm/docs/compilation/compile_models.html#compile-command-specification) for now. We will update documentation for `BuildArgs` soon. (Or you could look at the source code)"
      ],
      "metadata": {
        "id": "PpZGTy6QgZCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "build_args = mlc_llm.BuildArgs(\n",
        "    model=\"Llama-2-7b-chat-hf\",\n",
        "    quantization=\"q4f16_1\",\n",
        "    target=\"cuda\")"
      ],
      "metadata": {
        "id": "ei3r2kenNqbF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`mlc_llm.build_model` is the main entrance here. It takes in a `BuildArgs` to start the entire model compilation workflow.\n",
        "\n",
        "**Google Colab** If you are using Colab, the line below may require more RAM than the default Colab provides. You may need to either upgrade to a paid Colab plan, or run it in other environments. (Or sometimes, when you keep rerunning, (just the build portion), it eventually builds without exceeding the RAM Colab provides)"
      ],
      "metadata": {
        "id": "OIX3oyPhnCQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of `lib_path, model_path, chat_config_path = mlc_llm.build_model(build_args)` is given as a tuple of three paths.\n",
        "\n",
        "`lib_path` is the path to the specific binary that has been built.\n",
        "\n",
        "`model_path` is the path to the folder containing the compiled model parameters and other model specific configuration needed for other `mlc` modules.\n",
        "\n",
        "`chat_config_path` is the path to the specific `.json` configuration needed to have this model work with `mlc_chat`."
      ],
      "metadata": {
        "id": "F432oSPn2HSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lib_path, model_path, chat_config_path = mlc_llm.build_model(build_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMepfzs8O48L",
        "outputId": "d5ddedd1-c871-418d-8af1-d77bf6ae0508"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using path \"Llama-2-7b-chat-hf\" for model \"Llama-2-7b-chat-hf\"\n",
            "WARNING: --db-path does not point to a valid database: []\n",
            "Target configured: cuda -keys=cuda,gpu -arch=sm_75 -max_num_threads=1024 -thread_warp_size=32\n",
            "Automatically using target for weight quantization: cuda -keys=cuda,gpu -arch=sm_75 -max_num_threads=1024 -max_shared_memory_per_block=49152 -max_threads_per_block=1024 -registers_per_block=65536 -thread_warp_size=32\n",
            "Start computing and quantizing weights... This may take a while.\n",
            "Finish computing and quantizing weights.\n",
            "Total param size: 3.5313796997070312 GB\n",
            "Start storing to cache dist/Llama-2-7b-chat-hf-q4f16_1/params\n",
            "[0327/0327] saving param_326\n",
            "All finished, 115 total shards committed, record saved to dist/Llama-2-7b-chat-hf-q4f16_1/params/ndarray-cache.json\n",
            "Save a cached module to dist/Llama-2-7b-chat-hf-q4f16_1/mod_cache_before_build_cuda.pkl.\n",
            "Finish exporting to dist/Llama-2-7b-chat-hf-q4f16_1/Llama-2-7b-chat-hf-q4f16_1-cuda.so\n",
            "Finish exporting chat config to dist/Llama-2-7b-chat-hf-q4f16_1/params/mlc-chat-config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we can chat!\n",
        "\n",
        "Now we can chat using `mlc_chat`'s `ChatModule`. Note that `mlc_llm.build_model` returns the path to the generated files, and we can directly pass them in to the workflow below.\n",
        "\n",
        "For more details on `ChatModule`, please see the other tutorial [Getting Started with MLC-LLM](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb), or its documentation [here](https://mlc.ai/mlc-llm/docs/deploy/python.html#api-reference)."
      ],
      "metadata": {
        "id": "nEqk6mUXg70o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directly use the returned paths to launch `ChatModule`\n",
        "lib = tvm.runtime.load_module(lib_path)\n",
        "chat_mod = mlc_chat.ChatModule(target=\"cuda\")\n",
        "chat_mod.reload(lib=lib, model_path=model_path)"
      ],
      "metadata": {
        "id": "xtO-NmxpWqkx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "prompt = \"Prompt: Write me a poem about the city Pittsburgh\"\n",
        "chat_mod.prefill(input=prompt)\n",
        "\n",
        "msg = None\n",
        "while not chat_mod.stopped():\n",
        "    chat_mod.decode()\n",
        "    msg = chat_mod.get_message()\n",
        "    clear_output()\n",
        "    print(msg, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWhgHdNXXHW5",
        "outputId": "99b827d6-8b9d-4ebf-e0f6-f797609c5da2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! Here is a poem about the city of Pittsburgh:\n",
            "Pittsburgh, city of steel and might,\n",
            "Where the rivers flow and the bridges take flight.\n",
            "A city of contrasts, where the skyline meets the night,\n",
            "A place of beauty, where the grit and grime take flight.\n",
            "From the hills to the valleys, the views are simply divine,\n",
            "A city that's rich in history, and full of life.\n",
            "From the Steelers to the Pirates, and the Penguins too,\n",
            "Pittsburgh's sports teams have won the hearts of many a crew.\n",
            "The cultural scene is thriving, with art and music in the air,\n",
            "From the Carnegie Museum to the Mattress Factory, and the theaters beyond compare.\n",
            "The people of Pittsburgh, a melting pot of cultures and hues,\n",
            "A city that's welcoming, and full of surprises, and news.\n",
            "From the Strip District to the North Side, and everywhere in between,\n",
            "Pittsburgh's neighborhoods are full of life, and stories untold and unseen.\n",
            "So here's to Pittsburgh, a city of character and grace,\n",
            "A place that's home to many, and a city that's in its place.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PUxfyys4r7MW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}