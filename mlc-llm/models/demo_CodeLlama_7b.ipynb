{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IwhlCjVtpYj"
      },
      "source": [
        "# Demo: CodeLlama-7b with MLC LLM\n",
        "\n",
        "Recently, Meta unveiled [CodeLlama](https://github.com/facebookresearch/codellama), a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. This notebook demonstrates MLC LLM's support for the CodeLlama family:\n",
        "\n",
        "- **[CodeLlama](https://huggingface.co/codellama/CodeLlama-7b-hf): a coding foundation LLM**\n",
        "- **[CodeLlama-Instruct](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf): an instruction-tuned LLM for coding**\n",
        "- **[CodeLlama-Python](https://huggingface.co/codellama/CodeLlama-7b-Python-hf): a Python specialized LLM**\n",
        "\n",
        "In this respect, MLC LLM allows everyone to develop, optimize and deploy AI models natively on everyone's devices. Therefore, making possible the deployment of coding LLMs natively, acting as **a personal AI coding assistant**.\n",
        "\n",
        "In this notebook, we walk over the steps of using MLC LLM to run these pre-compiled CodeLlama models! We have uploaded various versions of the pre-compiled and quantized CodeLlama models here: https://huggingface.co/mlc-ai.\n",
        "\n",
        "Learn more about MLC LLM here: https://mlc.ai/mlc-llm/docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's an overview regarding each model's capabilities:\n",
        "\n",
        "|                       | Code Completion | Infilling | Instruction/chat | Python specialist |\n",
        "|-----------------------|-----------------|-----------|------------------|-------------------|\n",
        "| CodeLlama-7b          |        X        |     X     |                  |                   |\n",
        "| CodeLlama-7b-Python   |        X        |           |                  |         X         |\n",
        "| CodeLlama-7b-Instruct |        X        |     X     |         X        |                   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsvAL7SSt9Lo"
      },
      "source": [
        "Click the button below to get started!\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/models/demo_CodeLlama_7b.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kkADAMCCLi-"
      },
      "source": [
        "## Install MLC LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2EwuS6TCO61"
      },
      "source": [
        "We will start from setting up the environment. First, let us create a new Conda environment, in which we will run the rest of the notebook.\n",
        "\n",
        "```\n",
        "conda create --name mlc-llm python=3.10\n",
        "conda activate mlc-llm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojEeEmsqCTPG"
      },
      "source": [
        "**Google Colab**\n",
        "\n",
        "- If you are running this in a Google Colab notebook, you would not need to create a conda environment.\n",
        "- However, be sure to change your runtime to GPU by going to `Runtime` > `Change runtime type` and setting the Hardware accelerator to be \"GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_rX53bGChPn"
      },
      "source": [
        "If you are using CUDA, you can run the following command to confirm that CUDA is set up correctly, and check the driver version number as well as what GPUs are currently available for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRPeCflbCij6"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQfVfTAYC1M-"
      },
      "source": [
        "Next, let's download the MLC-AI and MLC-Chat nightly build packages. If you are running in a Colab environment, then you can just run the following command. Otherwise, go to https://mlc.ai/package/ and replace the command below with the one that is appropriate for your hardware and OS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi-udt4tC5c9"
      },
      "source": [
        "**Google Colab**: If you are using Colab, you may see the red warnings such as \"You must restart the runtime in order to use newly installed versions.\" For our purpose, we can disregard them, the notebook will still run correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah9tYaCRCkKS"
      },
      "outputs": [],
      "source": [
        "!pip install --pre --force-reinstall mlc-ai-nightly-cu118 mlc-chat-nightly-cu118 -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZGVNJE-DJ9E"
      },
      "source": [
        "Let's confirm we have installed the packages successfully!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6LszJgC7SQ"
      },
      "outputs": [],
      "source": [
        "!python -c \"import tvm; print('tvm installed properly!')\"\n",
        "!python -c \"import mlc_chat; print('mlc_chat installed properly!')\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGfnrRa9DMw1"
      },
      "source": [
        "## Download Prebuilt Models and Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVYkLb0eDjMi"
      },
      "source": [
        "The following commands will download all the available prebuilt libraries (e.g., `.so` files), including the precompiled CodeLlama models. This may take a while. If in **Google Colab**, you can verify that the files are being downloaded by clicking on the folder icon on the left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg7daEvlD5UB"
      },
      "source": [
        "Note: If you are NOT running in **Google Colab** you may need to run this line `!conda install git git-lfs` to install `git` and `git-lfs` before running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDFbw1KPDLu1"
      },
      "outputs": [],
      "source": [
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYqaVjmND7Px"
      },
      "outputs": [],
      "source": [
        "!mkdir -p dist/prebuilt\n",
        "!git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMEavWCJEC_d"
      },
      "source": [
        "#### CodeLlama-7b q4f16_1 prebuilt weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etHEUrfMD8bX"
      },
      "outputs": [],
      "source": [
        "!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-7b-hf-q4f16_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQkIX4TpElR6"
      },
      "source": [
        "#### CodeLlama-7b-Instruct q4f16_1 prebuilt weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTEGXAlhEnOw"
      },
      "outputs": [],
      "source": [
        "!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-7b-Instruct-hf-q4f16_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb1HpKpEoca"
      },
      "source": [
        "#### CodeLlama-7b-Python q4f16_1 prebuilt weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oysLKcZ4Eou7"
      },
      "outputs": [],
      "source": [
        "!cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-7b-Python-hf-q4f16_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbHdyfIXHNpo"
      },
      "outputs": [],
      "source": [
        "# Restart colab\n",
        "exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpxrrqyE0S6"
      },
      "source": [
        "## Let's code with CodeLlama!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y73vNLy1OfMr"
      },
      "source": [
        "Let's first try a simple code completion task with the CodeLlama-Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOEf8sDyEwuv"
      },
      "outputs": [],
      "source": [
        "from mlc_chat import ChatModule\n",
        "from mlc_chat.callback import StreamToStdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBcQMm-KJPN-"
      },
      "outputs": [],
      "source": [
        "codellama_python = ChatModule(model=\"CodeLlama-7b-Python-hf-q4f16_1\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g2l_hJhLyYm"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\\\n",
        "# Self-attention block implementation\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(\"\"\"\n",
        "\n",
        "output = codellama_python.generate(\n",
        "    prompt=prompt,\n",
        "    progress_callback=StreamToStdout(callback_interval=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwU54BtKQKz4",
        "outputId": "d5409224-cfbf-4c28-8a81-40bccfc02572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Self-attention block implementation\n",
            "class SelfAttentionBlock(nn.Module):\n",
            "    def __init__(self, dim, num_heads):\n",
            "        super().__init__()\n",
            "        self.num_heads = num_heads\n",
            "        self.key = nn.Linear(dim, dim))\n",
            "        self.value = nn.Linear(dim, dim))\n",
            "        self.proj = nn.Linear(dim, dim))\n",
            "\n",
            "    def forward(self, x):\n",
            "        B, N, C = x.shape\n",
            "        q = self.key(x[:, :, :-64])))\n",
            "        k = self.key(x[:, :, :64]]))\n",
            "        v = self.value(x[:, :, :]]]]))\n",
            "        attn = (q @ k.transpose(-1), v))\n",
            "\n",
            "        x = self.proj(attn[0]]))))\n",
            "\n",
            "        return x\n"
          ]
        }
      ],
      "source": [
        "print(prompt+output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFrVdqewL7_c"
      },
      "outputs": [],
      "source": [
        "# Restart colab to initialize a new ChatModule\n",
        "exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CUo34QeQto2"
      },
      "source": [
        "The CodeLlama models support infilling based on surrounding content. Let's try it with the foundation CodeLlama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeHn8je9SBpK"
      },
      "outputs": [],
      "source": [
        "from mlc_chat import ChatModule\n",
        "from mlc_chat.callback import StreamToStdout\n",
        "\n",
        "def text_infilling(prompt: str):\n",
        "    prefix = prompt.split(\"<FILL>\")[0]\n",
        "    suffix = prompt.split(\"<FILL>\")[1]\n",
        "    return f\"<PRE> {prefix} <SUF>{suffix} <MID>\"\n",
        "\n",
        "def print_infilling(prompt: str, output: str):\n",
        "    print(prompt.replace(\"<FILL>\", output.replace(\"<EOT>\", \"\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUU8zTdRSKNK"
      },
      "outputs": [],
      "source": [
        "codellama = ChatModule(model=\"CodeLlama-7b-hf-q4f16_1\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3rrm1qPSOkr"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\\\n",
        "# Installation instructions:\n",
        "    <FILL>\n",
        "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
        "\"\"\"\n",
        "\n",
        "output = codellama.generate(\n",
        "    prompt=text_infilling(prompt),\n",
        "    progress_callback=StreamToStdout(callback_interval=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78SHkqohUbCc",
        "outputId": "33a4ef28-2db0-4e8a-c886-a630fb2d8df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Installation instructions:\n",
            "    1. Clone the repository.\n",
            "    ```\n",
            "    git clone https://github.com/LLaMA/LLaMA.git\n",
            "    ```\n",
            "\n",
            "\n",
            "2. Install the pip package.\n",
            "    ```\n",
            "    cd LLaMA\n",
            "    pip install -e .\n",
            "    ```\n",
            " \n",
            "This downloads the LLaMA inference code and installs the repository as a local pip package.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_infilling(prompt, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKYyRyUVPGm2"
      },
      "outputs": [],
      "source": [
        "# Restart colab to create a new ChatModule\n",
        "exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqmYlUtwV01m"
      },
      "source": [
        "Finally, the CodeLlama-Instruct has instruction following ability for programming tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcCPRp9oWBfh"
      },
      "outputs": [],
      "source": [
        "from mlc_chat import ChatModule\n",
        "from mlc_chat.callback import StreamToStdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4C8iC9IWyZR"
      },
      "outputs": [],
      "source": [
        "codellama_instruct = ChatModule(model=\"CodeLlama-7b-Instruct-hf-q4f16_1\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kL02gKWW7Ov",
        "outputId": "26c64719-dc61-4f31-dd5d-78e215855197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a C++ program that computes the set of sums of all contiguous sublists of a given list:\n",
            "#include <iostream>\n",
            "using namespace std;\n",
            "void computeSums(const list<int> &lst, list<int> &sums) {\n",
            "    // Initialize the sums list\n",
            "    sums.clear();\n",
            "    // Compute the sums of all contiguous sublists\n",
            "    for (int i = 0; i < lst.size() - 1; i++) {\n",
            "        int sum = 0;\n",
            "        for (int j = i; j < lst.size() - 1; j++) {\n",
            "            sum += lst[j];\n",
            "        }\n",
            "        sums.push_back(sum));\n",
            "    }\n",
            "    // Print the sums list\n",
            "    for (int i = 0; i < sums.size(); i++) {\n",
            "        cout << sums[i] << endl;\n",
            "    }\n",
            "}\n",
            "int main() {\n",
            "    list<int> lst = {1, 2, 3, 4, 5};\n",
            "    list<int> sums;\n",
            "    computeSums(lst, sums);\n",
            "    return 0;\n",
            "}\n",
            "This program takes a list of integers as input, and computes the set of sums of all contiguous sublists of the input list. The program then prints the computed set of sums.\n",
            "Note that the input list must be a list of integers, and that the program will produce an error if the input list is not a list of integers.\n"
          ]
        }
      ],
      "source": [
        "prompt = (\"Write a C++ program that computes the set of sums of all contiguous\"\n",
        "          \"sublists of a given list.\")\n",
        "\n",
        "output = codellama_instruct.generate(\n",
        "    prompt=prompt,\n",
        "    progress_callback=StreamToStdout(callback_interval=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liWTlsBBYTRa",
        "outputId": "07907456-2600-4515-fc2d-af9213803ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the C++ program written in Java instead:\n",
            "import java.util.ArrayList;\n",
            "public class SumsOfSublists {\n",
            "    public static void main(String[] args) {\n",
            "        ArrayList<Integer> lst = new ArrayList<Integer>(){{add(1);add(2);add(3);add(4);add(5);}};\n",
            "        ArrayList<Integer> sums = new ArrayList<Integer>();\n",
            "        computeSums(lst, sums));\n",
            "        for (int i = 0; i < sums.size(); i++) {\n",
            "            System.out.println(sums[i])));\n",
            "        }\n",
            "    }\n",
            "    public static void computeSums(ArrayList<Integer> lst, ArrayList<Integer> sums) {\n",
            "        for (int i = 0; i < lst.size() - 1; i++) {\n",
            "            int sum = 0;\n",
            "            for (int j = i; j < lst.size() - 1; j++) {\n",
            "                sum += lst[j]);\n",
            "            }\n",
            "            sums.add(sum));\n",
            "        }\n",
            "    }\n",
            "}\n",
            "This Java program takes a list of integers as input, and computes the set of sums of all contiguous sublists of the input list. The program then prints the computed set of sums.\n",
            "Note that the input list must be a list of integers, and that the program will produce an error if the input list is not a list of integers.\n"
          ]
        }
      ],
      "source": [
        "output = codellama_instruct.generate(\n",
        "    prompt=\"Write this in Java instead.\",\n",
        "    progress_callback=StreamToStdout(callback_interval=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzIMwuiaWwSg"
      },
      "outputs": [],
      "source": [
        "# Restart colab to create a new ChatModule\n",
        "exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMgGy5PkxU2LXzQzjaEnyyL",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
